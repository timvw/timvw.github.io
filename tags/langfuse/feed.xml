<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Langfuse on Tim Van Wassenhove</title>
    <link>https://timvw.be/tags/langfuse/</link>
    <description>Recent content in Langfuse on Tim Van Wassenhove</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Jun 2025 00:28:03 +0200</lastBuildDate>
    <atom:link href="https://timvw.be/tags/langfuse/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Distributed Tracing with FastMCP: Combining OpenTelemetry and Langfuse</title>
      <link>https://timvw.be/2025/06/27/distributed-tracing-with-fastmcp-combining-opentelemetry-and-langfuse/</link>
      <pubDate>Fri, 27 Jun 2025 10:00:00 +0100</pubDate>
      <guid>https://timvw.be/2025/06/27/distributed-tracing-with-fastmcp-combining-opentelemetry-and-langfuse/</guid>
      <description>&lt;p&gt;When building distributed AI applications with &lt;a href=&#34;https://github.com/jlowin/fastmcp&#34;&gt;FastMCP&lt;/a&gt;, proper observability becomes crucial. In this post, I&amp;rsquo;ll show you how to combine &lt;a href=&#34;https://opentelemetry.io/&#34;&gt;OpenTelemetry&lt;/a&gt; context propagation with &lt;a href=&#34;https://langfuse.com/&#34;&gt;Langfuse&lt;/a&gt; to achieve seamless distributed tracing across your MCP client and server components.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-challenge&#34;&gt;The Challenge&lt;/h2&gt;&#xA;&lt;p&gt;Modern AI applications often consist of multiple services communicating over the network. When using FastMCP to build Model Context Protocol servers, we need to:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Track requests as they flow from client to server&lt;/li&gt;&#xA;&lt;li&gt;Monitor LLM interactions and their performance&lt;/li&gt;&#xA;&lt;li&gt;Maintain trace context across service boundaries&lt;/li&gt;&#xA;&lt;li&gt;Correlate logs and metrics with specific request traces&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;the-solution-opentelemetry--langfuse&#34;&gt;The Solution: OpenTelemetry + Langfuse&lt;/h2&gt;&#xA;&lt;p&gt;By combining &lt;a href=&#34;https://opentelemetry.io/&#34;&gt;OpenTelemetry&lt;/a&gt;&amp;rsquo;s distributed tracing capabilities with &lt;a href=&#34;https://langfuse.com/&#34;&gt;Langfuse&lt;/a&gt;&amp;rsquo;s LLM-specific observability features, we can achieve comprehensive monitoring of our AI applications.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
